{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"grpo_tweak_dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 rows from 4.jsonl\n",
      "Loaded 12 rows from 6.jsonl\n",
      "Loaded 12 rows from 2.jsonl\n",
      "Loaded 12 rows from 0.jsonl\n",
      "Loaded 12 rows from 7.jsonl\n",
      "Loaded 12 rows from 5.jsonl\n",
      "Loaded 12 rows from 1.jsonl\n",
      "Loaded 12 rows from 3.jsonl\n",
      "Created DataFrame with 96 rows from 8 files\n"
     ]
    }
   ],
   "source": [
    "def load_all_jsonl_to_dataframe(folder_path):\n",
    "    \"\"\"Load all JSONL files from a folder into a pandas DataFrame.\"\"\"\n",
    "    # Find all .jsonl files\n",
    "    jsonl_files = glob.glob(os.path.join(folder_path, \"*.jsonl\"))\n",
    "    \n",
    "    # Create list to hold dataframes\n",
    "    dfs = []\n",
    "    \n",
    "    for file_path in jsonl_files:\n",
    "        try:\n",
    "            # Read file into dataframe\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "            df['source_file'] = os.path.basename(file_path)  # Optional: track source file\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {os.path.basename(file_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Created DataFrame with {len(combined_df)} rows from {len(jsonl_files)} files\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No valid files found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Usage\n",
    "folder_path = \"../data/DeepScaleR_1_5_B_results/\"\n",
    "combined_data = load_all_jsonl_to_dataframe(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "# combined_data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Extraction Utilities\n",
    "\n",
    "Functions to extract solutions from model outputs and compute correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_boxed(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    left = \"\\\\boxed{\"\n",
    "\n",
    "    if s[:len(left)] != left:\n",
    "        return None\n",
    "    if s[-1] != \"}\":\n",
    "        return None\n",
    "\n",
    "    return s[len(left):-1]\n",
    "\n",
    "\n",
    "def last_boxed_only_string(string):\n",
    "    idx = string.rfind(\"\\\\boxed\")\n",
    "    if \"\\\\boxed \" in string:\n",
    "        return \"\\\\boxed \" + string.split(\"\\\\boxed \")[-1].split(\"$\")[0]\n",
    "    if idx < 0:\n",
    "        idx = string.rfind(\"\\\\fbox\")\n",
    "        if idx < 0:\n",
    "            return None\n",
    "\n",
    "    i = idx\n",
    "    right_brace_idx = None\n",
    "    num_left_braces_open = 0\n",
    "    while i < len(string):\n",
    "        if string[i] == \"{\":\n",
    "            num_left_braces_open += 1\n",
    "        if string[i] == \"}\":\n",
    "            num_left_braces_open -= 1\n",
    "            if num_left_braces_open == 0:\n",
    "                right_brace_idx = i\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    if right_brace_idx is None:\n",
    "        retval = None\n",
    "    else:\n",
    "        retval = string[idx:right_brace_idx + 1]\n",
    "\n",
    "    return retval\n",
    "\n",
    "def extract_solution(text):\n",
    "    return remove_boxed(last_boxed_only_string(text))\n",
    "\n",
    "def correctness_reward_func(response: str, actual_answers: str) -> float:\n",
    "    extracted_answer = extract_solution(response)\n",
    "    return 1.0 if extracted_answer == actual_answers else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = combined_data.copy()[['prompt', 'answer']].drop_duplicates()\n",
    "# print(f\"len(check): {len(check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = combined_data.copy()\n",
    "df['question_text'] = df['prompt']\n",
    "df['ground_truth'] = df['answer']\n",
    "df.drop([\n",
    "    'source_file',\n",
    "], axis=1, inplace=True)\n",
    "# print(f\"len(df): {len(df)}\")\n",
    "df = df.explode('outputs')\n",
    "df['response'] = df['outputs'].apply(lambda x: x['output'])\n",
    "# print(f\"len(df): {len(df)}\")\n",
    "df['extracted_solution'] = df['response'].apply(lambda x: extract_solution(x))\n",
    "df['response_word_count'] = df['response'].astype(str).str.split().str.len()\n",
    "\n",
    "# # # Only keep completions where there was a generated solution.\n",
    "# # df = df[df['extracted_solution'].notna()]\n",
    "\n",
    "df['reward'] = (df['extracted_solution'] == df['ground_truth'].astype(str)).astype(float)\n",
    "# df\n",
    "# df[df['extracted_solution'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_at_k(n, c, k):\n",
    "    \"\"\"\n",
    "    :param n: total number of samples :param c: number of correct\n",
    "    Samples\n",
    "    :param k: k in pass@$k$\n",
    "    \"\"\"\n",
    "    if n - c < k: return 1.0\n",
    "    return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n",
    "\n",
    "# pass_at_k(16, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(perf): 87\n"
     ]
    }
   ],
   "source": [
    "perf = (\n",
    "    df[['question_text', 'ground_truth', 'reward']]\n",
    "    .groupby(['question_text', 'ground_truth'])\n",
    "    .agg(\n",
    "        count=('reward', 'count'),\n",
    "        sum_reward=('reward', 'sum'),\n",
    "        accuracy=('reward', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"len(perf): {len(perf)}\")\n",
    "\n",
    "# If only answer with an extarcted \\boxed{...} answer are kept, count isn't always 16.\n",
    "# assert(len(perf[perf['count'] != 16]) == 0)\n",
    "# perf[f'pass@{k}'] = perf['sum_reward'].apply(lambda x: pass_at_k(16, x, k))\n",
    "\n",
    "k = 8\n",
    "perf[f'pass@{k}'] = perf.apply(\n",
    "    lambda x: pass_at_k(x['count'], x['sum_reward'], k), \n",
    "    axis=1\n",
    ")\n",
    "# perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_perf = perf[[f'pass@{k}']].mean()\n",
    "# overall_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 87\n",
      "total_too_hard: 38\n",
      "total_ripe: 16\n",
      "total_already_done: 33\n"
     ]
    }
   ],
   "source": [
    "total = len(perf)\n",
    "print(f\"total: {total}\")\n",
    "\n",
    "# Too hard.\n",
    "too_hard = perf[(perf['accuracy'] == 0.0)]\n",
    "total_too_hard = len(too_hard)\n",
    "print(f\"total_too_hard: {total_too_hard}\")\n",
    "\n",
    "# Ripe for improvement\n",
    "ripe = perf[(perf['accuracy'] <= 0.8) & (perf['accuracy'] > 0.0)]\n",
    "total_ripe = len(ripe)\n",
    "print(f\"total_ripe: {total_ripe}\")\n",
    "\n",
    "# Not worth improving.\n",
    "already_done = perf[(perf['accuracy'] >= 0.8)]\n",
    "total_already_done = len(already_done)\n",
    "print(f\"total_already_done: {total_already_done}\")\n",
    "\n",
    "check_total = total_too_hard + total_ripe + total_already_done\n",
    "assert(total == check_total)\n",
    "\n",
    "# ripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweak_dataset = [\n",
    "    {\n",
    "        'problem': r['question_text'],\n",
    "        'answer': str(r['ground_truth']),\n",
    "    }\n",
    "    for index, r in ripe.reset_index().iterrows()\n",
    "]\n",
    "# len(tweak_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16 records to grpo_tweak_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Create directory if it doesn't exist\n",
    "dirname = os.path.dirname(OUTPUT_FILE)\n",
    "if len(dirname.strip()) > 0:\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "# Save to JSONL file\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    for item in tweak_dataset:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(f\"Saved {len(tweak_dataset)} records to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strat",
   "language": "python",
   "name": "strat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
